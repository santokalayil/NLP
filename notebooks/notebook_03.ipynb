{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a real life use case and see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Camtasia Support Ticketing system: \n",
    "\n",
    "\n",
    "**Customer Complain Severity Classification System**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    raw_text[Camtasia 10.0 won't import mp4 file through import media section]\n",
    "    raw_text -- [TF-IDF Vectorizer] --> vector[0.36<br> 0.67<br>0.299<br>0<br>0<br>0.56<br>...<br>0.44]\n",
    "    vector --> classifier{Naive Bayes Classifier} --> High & Medium & Low\n",
    "    High -.-> CustomerCareCall\n",
    "    Low -.-> CreateJIRATicket\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Pipeline _flow\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    DA[Data Aquisition] --> TD[Raw data] --discarding irrelavant information--> ND[Select only needed data with labels]\n",
    "    daug[Data Augmentation] -.if not data available.-> CD\n",
    "    CD[Cleaned Data] -.-> X[Text] & y[Label]\n",
    "    X-.- ST[Sentence Augumentation/Tokenization -split into sentences]-.-> sent1[Sentence 1] & sent2[Sentence 2] & sentx[Sentence x]\n",
    "    \n",
    "```\n",
    "\n",
    "Ready-made sentence tokenizer can be aviable in NLTK or in Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    sent1 -.-> WT(word Tokenization) -.-> word1[eat] & word2[eating] & word3[cat] & wordx[ate]\n",
    "    word1 -.stemming.-> stem1[eat]\n",
    "    word2 -.stemming.-> stem2[eat]\n",
    "    word3 -.stemming.-> stem3[cat]\n",
    "    wordx -.stemming.-> stemx[ate]\n",
    "\n",
    "    stem1--lemmatization--> lem1[eat]\n",
    "    stem2--lemmatization--> lem2[eat]\n",
    "    stem3--lemmatization--> lem3[cat]\n",
    "    stemx--lemmatization--> lemx[eat]\n",
    "```\n",
    "- Until here, it is called `pre-processing`\n",
    "- This is process of `word tokenization` is repeated for each sentences.\n",
    "- stemming won't convert `ate` to `eat`. There we need `lemmatization`\n",
    "- `stemming` & `lemmatization` are needed to map a word to its base word\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "- adjustable -> adjust\n",
    "- formality -> formaliti\n",
    "- formaliti ->formal\n",
    "- airliner -> airlin\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "- was -> (to) be\n",
    "- better -> good\n",
    "- meeting --> meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more understaing, we are changing lematized words example in the follwoing diagrm to some different ocnes\n",
    "```mermaid\n",
    "graph LR;\n",
    "    preprocess[Stemmed and Lematized Words] --> lw1(camtasia) & lw2(close) & lw3(crash) & lwx(...)\n",
    "    lw1 & lw2 & lw3 & lwx --> FE[feature engineering]--> vector[0.36<br>0.67<br>0.299<br>0<br>0<br>0.56<br>...<br>0.44]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Diagram\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph LR;\n",
    "    sent(Camtasia 10.0<br> won't import<br> mp4 file <br>through <br>import media section)\n",
    "    sent-->tfidf[TF-IDF]--> vector[0.36<br>0.67<br>0.299<br>0<br>0<br>0.56<br>...<br>0.44]\n",
    "    vector-->ml_model[Naive Bayes Classifier or others] --> high & medium & low\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    DA[data aquisition] --> TEC[Text Extraction <br> & Cleanup] --> PP[Pre-processing]--> FE[Feature Engineering]\n",
    "    FE --> MB[Model Building] -->md1[random forest model 1] & md2[logistic regresssion model 2] & md3[Naive Bayes model3]\n",
    "    md1 & md2 & md3 --> HPT[Hyper Parameter Tuning like grid search cv]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix --> [Accuracy / Precision / Recall / F1Score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    DA[data aquisition] --> TEC[Text Extraction <br> & Cleanup] --> PP[Pre-processing]--> FE[Feature Engineering]\n",
    "    FE--> MB[Model Building] --> Ev[Evaluation]--> DP[Deployment] --> MU[Monitor & update]\n",
    "    MU -.-> DA\n",
    "    Ev -.-> PP\n",
    "```\n",
    "This is an iterative process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    A--> B & C & D;\n",
    "    B--> A & E;\n",
    "    C--> A & E;\n",
    "    D--> A & E;\n",
    "    E--> B & C & D;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    A--> B & C & D;\n",
    "    B--> A & E;\n",
    "    C--> A & E;\n",
    "    D--> A & E;\n",
    "    E--> B & C & D;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp--9r02_gx-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
